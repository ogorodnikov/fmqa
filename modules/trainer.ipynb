{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bfe57fb-2182-4a4f-be0b-dae2bbe69baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "    \n",
    "    def __init__(self, dataset, shuffle=True, \n",
    "                 train_rate=0.8, valid_rate=0.1, dataset_share=1,\n",
    "                 batch_size=2048, dataloader_workers_count=0):        \n",
    "                   \n",
    "        dataset_length = len(dataset)\n",
    "\n",
    "        train_length = int(dataset_length * train_rate)\n",
    "        valid_length = int(dataset_length * valid_rate)\n",
    "\n",
    "        test_length = dataset_length - train_length - valid_length\n",
    "\n",
    "        dataset_indices = np.arange(dataset_length, dtype=int)\n",
    "        \n",
    "        if shuffle:        \n",
    "            np.random.shuffle(dataset_indices)\n",
    "        \n",
    "        train_end = int(train_length * dataset_share)\n",
    "        valid_end = int(train_end + valid_length * dataset_share)\n",
    "        test_end = int(valid_end + test_length * dataset_share)\n",
    "        \n",
    "        index_ranges = np.split(dataset_indices, (train_end, valid_end, test_end))[:3]     \n",
    "            \n",
    "        train_sampler, valid_sampler, test_sampler = map(SequentialValueSampler, index_ranges)\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.lengths = (train_length, valid_length, test_length)\n",
    "        \n",
    "        dataloader = torch.utils.data.DataLoader\n",
    "\n",
    "        self.train_data_loader = dataloader(dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            sampler=train_sampler,\n",
    "                                            num_workers=dataloader_workers_count)\n",
    "        \n",
    "        self.valid_data_loader = dataloader(dataset,                                  \n",
    "                                            batch_size=batch_size,\n",
    "                                            sampler=valid_sampler,\n",
    "                                            num_workers=dataloader_workers_count)\n",
    "        \n",
    "        self.test_data_loader = dataloader(dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           sampler=test_sampler,\n",
    "                                           num_workers=dataloader_workers_count)\n",
    "\n",
    "        \n",
    "class SequentialValueSampler(torch.utils.data.Sampler):\n",
    "\n",
    "    def __init__(self, values):\n",
    "        self.values = values\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eac777c7-6a93-4b01-9e36-0d1a58d6906e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self,\n",
    "                 splitter,\n",
    "                 model=None, \n",
    "                 metric=None, \n",
    "                 optimizer=None,\n",
    "                 criterion=None,\n",
    "                 learning_rate=0.001, \n",
    "                 weight_decay=1e-6,\n",
    "                 embedding_dimensions=16,\n",
    "                 device='cpu'): \n",
    "        \n",
    "        field_dimensions = splitter.dataset.field_dimensions\n",
    "        \n",
    "        model = model or OneHotFactorizationMachine(field_dimensions=field_dimensions,\n",
    "                                                    embedding_dimensions=embedding_dimensions)\n",
    "        \n",
    "        metric = metric or sklearn.metrics.r2_score\n",
    "\n",
    "        optimizer = optimizer or torch.optim.Adam(params=model.parameters(), \n",
    "                                                  lr=learning_rate, \n",
    "                                                  weight_decay=weight_decay)\n",
    "\n",
    "        criterion = criterion or torch.nn.MSELoss()  \n",
    "        \n",
    "        self.epochs_passed = 0\n",
    "        \n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        self.model = model.to(device)\n",
    "        self.metric = metric\n",
    "        self.splitter = splitter\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        self.validation_scores = []\n",
    "        \n",
    "        # TODO: EarlyStopper\n",
    "    \n",
    "    \n",
    "    def train(self,\n",
    "              epochs=10, \n",
    "              validate=True,\n",
    "              disable_progressbar_printout=False):        \n",
    "        \n",
    "        batch_size = self.splitter.batch_size\n",
    "        \n",
    "        train_batch_count = len(self.splitter.train_data_loader)\n",
    "        \n",
    "        records_count = epochs * train_batch_count * batch_size  \n",
    "        \n",
    "        fit_batch_tracker = tqdm.trange(\n",
    "            records_count,\n",
    "            unit=' records',\n",
    "            unit_scale=True,\n",
    "            # ascii=True,\n",
    "            ncols=110,\n",
    "            mininterval=1,\n",
    "            disable=disable_progressbar_printout,\n",
    "        )\n",
    "        \n",
    "        epochs_total = self.epochs_passed + epochs\n",
    "        \n",
    "        for epoch in range(epochs):         \n",
    "\n",
    "            fit_batch_tracker.set_description(f\"Epoch: {self.epochs_passed + 1}/{epochs_total}\")\n",
    "            \n",
    "            \n",
    "            # Train Part\n",
    "\n",
    "            interval_loss = 0\n",
    "\n",
    "            for batch, (fields, targets) in enumerate(self.splitter.train_data_loader):\n",
    "                \n",
    "                loss = self.fit(fields, targets)\n",
    "                \n",
    "                interval_loss += loss\n",
    "                \n",
    "                # print(batch)\n",
    "                \n",
    "                fit_batch_tracker.update(batch_size)                    \n",
    "            \n",
    "            \n",
    "            if validate:  \n",
    "                \n",
    "                validation_predictions, validation_score = self.batch_predict(self.splitter.valid_data_loader)                \n",
    "                \n",
    "                fit_batch_tracker.set_postfix(v=f\"{validation_score:.02f}\")\n",
    "\n",
    "                self.validation_scores.append(validation_score)\n",
    "                \n",
    "            self.epochs_passed += 1\n",
    "         \n",
    "        train_predictions, train_score = self.batch_predict(self.splitter.train_data_loader)\n",
    "        test_predictions, test_score = self.batch_predict(self.splitter.test_data_loader)\n",
    "        \n",
    "        print(f\"Train {self.metric.__name__}: {train_score:.02f}\")            \n",
    "        print(f\"Test  {self.metric.__name__}: {test_score:.02f}\")\n",
    "    \n",
    "    \n",
    "    def fit(self, fields, targets):\n",
    "\n",
    "        self.model.train()\n",
    "            \n",
    "        predictions = self.model(fields)\n",
    "            \n",
    "        loss = self.criterion(predictions, targets.float())\n",
    "            \n",
    "        self.model.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "                \n",
    "                \n",
    "    def batch_predict(self, data_loader):\n",
    "    \n",
    "        targets = []\n",
    "        predictions = []\n",
    "\n",
    "        for fields, target in data_loader:\n",
    "\n",
    "            prediction = self.predict(fields)\n",
    "\n",
    "            targets.extend(target.tolist())\n",
    "            predictions.extend(prediction.tolist())\n",
    "\n",
    "        score = self.metric(targets, predictions)\n",
    "        \n",
    "        return predictions, score\n",
    "    \n",
    "    \n",
    "    def predict(self, fields):\n",
    "    \n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            if not isinstance(fields, torch.Tensor):\n",
    "                \n",
    "                fields = torch.tensor(fields)\n",
    "\n",
    "            predictions = self.model(fields).numpy()\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d82125-133b-4776-9ec1-9d641be29e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

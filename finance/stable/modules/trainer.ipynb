{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19a8873b-72dc-4837-baa6-557cfbcd7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path):        \n",
    "        \n",
    "        # Data\n",
    "        \n",
    "        self.data = pd.read_csv(dataset_path)\n",
    "        \n",
    "        data = self.data.to_numpy()\n",
    "                        \n",
    "        self.fields = data[:, :-4].astype(int)\n",
    "        self.targets = data[:, -1]\n",
    "        \n",
    "        self.field_dimensions = np.max(self.fields, axis=0).astype(int) + 1\n",
    "        \n",
    "        self.field_dimensions[self.field_dimensions < 2] = 2\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.fields.shape[0]\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        fields = self.fields[index]\n",
    "        target = self.targets[index].squeeze()\n",
    "        \n",
    "        return fields, target\n",
    "    \n",
    "    def plot(self):\n",
    "\n",
    "        top_sharpe_index = self.data.sharpe.idxmax()\n",
    "\n",
    "        top_sharpe_row = self.data.iloc[top_sharpe_index]\n",
    "\n",
    "        top_return = top_sharpe_row['return']\n",
    "        top_risk = top_sharpe_row['risk']\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        plt.title('Markowitz portfolio (Combinations of all portfolio selections)')\n",
    "        plt.xlabel('Volatility - standard deviation')\n",
    "        plt.ylabel('Return')\n",
    "\n",
    "        plt.scatter(self.data.risk, self.data['return'], c=self.data.sharpe, cmap='viridis')\n",
    "        plt.scatter(top_risk, top_return, c='red', s=50, marker=5)  \n",
    "        plt.colorbar(label='Sharpe Ratio')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bfe57fb-2182-4a4f-be0b-dae2bbe69baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "    \n",
    "    def __init__(self, dataset, shuffle=True, \n",
    "                 train_rate=0.8, valid_rate=0.1, dataset_share=1,\n",
    "                 batch_size=2048, dataloader_workers_count=8):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "                    \n",
    "        dataset_length = len(dataset)\n",
    "\n",
    "        train_length = int(dataset_length * train_rate)\n",
    "        valid_length = int(dataset_length * valid_rate)\n",
    "\n",
    "        test_length = dataset_length - train_length - valid_length\n",
    "\n",
    "        dataset_indices = np.arange(dataset_length, dtype=int)\n",
    "        \n",
    "        if shuffle:        \n",
    "            np.random.shuffle(dataset_indices)\n",
    "        \n",
    "        train_end = int(train_length * dataset_share)\n",
    "        valid_end = int(train_end + valid_length * dataset_share)\n",
    "        test_end = int(valid_end + test_length * dataset_share)\n",
    "        \n",
    "        self.lengths = (train_end, valid_end, test_end)\n",
    "        \n",
    "        # print(\"train_length, train_end:\", train_length, train_end)\n",
    "        # print(\"valid_length, valid_end:\", valid_length, valid_end)\n",
    "        # print(\"test_length, test_end:\", test_length, test_end)\n",
    "        \n",
    "        index_ranges = np.split(dataset_indices, (train_end, valid_end, test_end))[:3]     \n",
    "            \n",
    "        train_sampler, valid_sampler, test_sampler = map(SequentialValueSampler, index_ranges)\n",
    "\n",
    "        self.train_data_loader = DataLoader(dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            sampler=train_sampler,\n",
    "                                            num_workers=dataloader_workers_count)\n",
    "        \n",
    "        self.valid_data_loader = DataLoader(dataset,                                  \n",
    "                                            batch_size=batch_size,\n",
    "                                            sampler=valid_sampler,\n",
    "                                            num_workers=dataloader_workers_count)\n",
    "        \n",
    "        self.test_data_loader = DataLoader(dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           sampler=test_sampler,\n",
    "                                           num_workers=dataloader_workers_count)\n",
    "\n",
    "        \n",
    "class SequentialValueSampler(torch.utils.data.Sampler):\n",
    "\n",
    "    def __init__(self, values):\n",
    "        self.values = values\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eac777c7-6a93-4b01-9e36-0d1a58d6906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, splitter,\n",
    "                 learning_rate=0.001, weight_decay=1e-6,\n",
    "                 embedding_dimensions=16,\n",
    "                 model=None, criterion=None, metric=None, optimizer=None,\n",
    "                 device='cpu', batch_logging_interval=100): \n",
    "        \n",
    "        criterion = criterion or torch.nn.MSELoss()   \n",
    "\n",
    "        metric = metric or sklearn.metrics.r2_score\n",
    "        \n",
    "        field_dimensions = splitter.dataset.field_dimensions\n",
    "        \n",
    "        model = model or CustomFactorizationMachine(field_dimensions, \n",
    "                                                    embedding_dimensions=embedding_dimensions)\n",
    "\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), \n",
    "                                     lr=learning_rate, \n",
    "                                     weight_decay=weight_decay)\n",
    "        \n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.metric = metric\n",
    "        self.optimizer = optimizer\n",
    "        self.splitter = splitter\n",
    "        \n",
    "        self.batch_logging_interval = batch_logging_interval\n",
    "        \n",
    "        self.train_data_loader = self.splitter.train_data_loader\n",
    "        self.valid_data_loader = self.splitter.valid_data_loader\n",
    "        self.test_data_loader = self.splitter.test_data_loader\n",
    "        \n",
    "        self.epoch_scores = []\n",
    "        \n",
    "        # TODO: EarlyStopper\n",
    "    \n",
    "    \n",
    "    def fit(self, epochs=10, \n",
    "            validate=True, test=True, \n",
    "            disable_progressbar_printout=False):\n",
    "        \n",
    "        epoch_offset = len(self.epoch_scores)\n",
    "        \n",
    "        batches_count = epochs * len(self.splitter.train_data_loader)       \n",
    "        \n",
    "        fit_batch_tracker = tqdm.trange(\n",
    "            batches_count,\n",
    "            unit=' batches',\n",
    "            ncols=110,\n",
    "            mininterval=1,\n",
    "            disable=disable_progressbar_printout,\n",
    "        ) \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            current_epoch = epoch_offset + epoch + 1\n",
    "            max_epochs = epoch_offset + epochs\n",
    "\n",
    "            fit_batch_tracker.set_description(f\"Epoch: {current_epoch}/{max_epochs}\")     \n",
    "            \n",
    "            self.train(fit_batch_tracker)\n",
    "            \n",
    "            validation_score = 0\n",
    "            \n",
    "            if validate:                \n",
    "                validation_score, validation_predictions = self.test(self.valid_data_loader)                \n",
    "                fit_batch_tracker.set_postfix(r2=f\"{validation_score:.02f}\")\n",
    "\n",
    "            self.epoch_scores.append(validation_score)\n",
    "                \n",
    "        if test:            \n",
    "            total_score, total_predictions = self.test(self.test_data_loader)        \n",
    "            print(f\"Test {self.metric.__name__}: {total_score:.05f}\")\n",
    "    \n",
    "    \n",
    "    def train(self, fit_batch_tracker):\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        interval_loss = 0\n",
    "\n",
    "        for batch, (fields, target) in enumerate(self.splitter.train_data_loader):\n",
    "            \n",
    "            predictions = self.model(fields)\n",
    "            \n",
    "            loss = self.criterion(predictions, target.float())\n",
    "            \n",
    "            self.model.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            interval_loss += loss.item()\n",
    "\n",
    "            if not (batch + 1) % self.batch_logging_interval:\n",
    "\n",
    "                average_loss = interval_loss / self.batch_logging_interval\n",
    "\n",
    "                # fit_batch_tracker.set_postfix(loss=f\"{average_loss:1.05f}\")\n",
    "                \n",
    "                fit_batch_tracker.update(self.batch_logging_interval)\n",
    "                \n",
    "                interval_loss = 0\n",
    "                \n",
    "                \n",
    "    def test(self, data_loader):\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "        targets = []\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for fields, target in data_loader:\n",
    "\n",
    "                prediction = self.model(fields)\n",
    "\n",
    "                targets.extend(target.tolist())\n",
    "                predictions.extend(prediction.tolist())\n",
    "\n",
    "        score = self.metric(targets, predictions)\n",
    "        \n",
    "        return score, predictions\n",
    "    \n",
    "\n",
    "    def predict(self, indices):   \n",
    "        \n",
    "        encoded_fields = self.splitter.dataset.fields[indices]\n",
    "        targets = self.splitter.dataset.targets[indices]\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            predictions = self.model(torch.tensor(encoded_fields))\n",
    "        \n",
    "        return targets, predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

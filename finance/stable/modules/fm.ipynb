{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8101d794-a97c-476f-b609-767c28e7181d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Factorization Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8cd253d-8856-4353-8e72-a67eee5765e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesEmbedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, field_dimensions, embedding_dimensions):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        fields_count = sum(field_dimensions)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(fields_count, embedding_dimensions)\n",
    "        \n",
    "        field_offsets = np.cumsum(field_dimensions)\n",
    "        \n",
    "        self.embedding_offsets = np.array((0, *field_offsets[:-1]), dtype=int)\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "        \n",
    "        # self.embedding.weight.data = self.embedding.weight.data / 10000\n",
    "        \n",
    "        # print(\"fields_count:\", fields_count)\n",
    "        # print(\"field_offsets:\", field_offsets)        \n",
    "        # print(\"self.embedding_offsets:\", self.embedding_offsets)\n",
    "        # print(\"self.embedding.weight.data:\", self.embedding.weight.data)              \n",
    "        \n",
    "    def forward(self, embedding_inputs):\n",
    "        \n",
    "        one_hot_offsets = embedding_inputs.new_tensor(self.embedding_offsets).unsqueeze(0)\n",
    "\n",
    "        one_hot_positions = embedding_inputs + one_hot_offsets\n",
    "        \n",
    "        embedding_weights = self.embedding(one_hot_positions)\n",
    "        \n",
    "#         print(\"embedding_inputs:\", embedding_inputs)        \n",
    "#         print(\"one_hot_positions:\", one_hot_positions)        \n",
    "#         print(\"embedding_weights:\", embedding_weights)\n",
    "        \n",
    "        return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2054ae5-a031-4a09-a9b6-1177f949b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachine(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, reduce_sum=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.reduce_sum = reduce_sum\n",
    "\n",
    "    def forward(self, weights):\n",
    "\n",
    "        square_of_sum = torch.sum(weights, dim=1) ** 2\n",
    "        sum_of_squares = torch.sum(weights ** 2, dim=1)\n",
    "        \n",
    "        prediction = square_of_sum - sum_of_squares           \n",
    "        \n",
    "        if self.reduce_sum:\n",
    "            prediction = torch.sum(prediction, dim=1, keepdim=True)\n",
    "        \n",
    "        prediction = 0.5 * prediction\n",
    "\n",
    "        # print(\"square_of_sum:\", square_of_sum)    \n",
    "        # print(\"sum_of_squares:\", sum_of_squares)               \n",
    "        # print(\"prediction:\", prediction)        \n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bb967ed-a446-4d00-b627-784e94740ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomFactorizationMachine(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, field_dimensions, embedding_dimensions):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = FeaturesEmbedding(field_dimensions, embedding_dimensions)\n",
    "        # self.linear = FeaturesLinear(field_dimensions)\n",
    "        self.fm = FactorizationMachine(reduce_sum=True)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # linear_out = self.linear(inputs)\n",
    "        \n",
    "        embedding_weights = self.embedding(inputs)\n",
    "        \n",
    "        fm_predictions = self.fm(embedding_weights)\n",
    "        \n",
    "        # result = (linear_out + fm_predictions).squeeze(1)\n",
    "        \n",
    "        result = fm_predictions.squeeze(1)\n",
    "        \n",
    "        # sigmoid_result = torch.sigmoid(squeezed_result)\n",
    "        \n",
    "        # print(\"inputs:\", inputs)    \n",
    "        # print(\"embedding_weights:\", embedding_weights)               \n",
    "        # print(\"fm_predictions:\", fm_predictions)  \n",
    "        # print(\"result:\", result)\n",
    "        \n",
    "        return result    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a943343d-c3ae-4ea6-b8e8-836be454a9ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ad61f-fd00-4096-9795-18391f530c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeaturesLinear(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self, field_dims, output_dim=1):\n",
    "        \n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\n",
    "#         self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
    "#         self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=int)\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        \n",
    "#         return torch.sum(self.fc(x), dim=1) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a77d4f-4dc5-4f47-8d09-2637c7b93f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CustomDeepFactorizationMachineModel(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self, field_dims, embed_dim, mlp_dims, dropout):\n",
    "        \n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.linear = FeaturesLinear(field_dims)\n",
    "#         self.fm = FactorizationMachine(reduce_sum=True)\n",
    "#         self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "#         self.embed_output_dim = len(field_dims) * embed_dim\n",
    "#         self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_dims, dropout)\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         embed_x = self.embedding(x)\n",
    "        \n",
    "#         result = self.linear(x) + self.fm(embed_x) + self.mlp(embed_x.view(-1, self.embed_output_dim))\n",
    "        \n",
    "#         squeezed_result = result.squeeze(1)\n",
    "        \n",
    "#         sigmoid_result = torch.sigmoid(squeezed_result)\n",
    "        \n",
    "#         return squeezed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a7034-d5f5-45de-8abd-ac82e908279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiLayerPerceptron(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self, input_dim, embed_dims, dropout, output_layer=True):\n",
    "        \n",
    "#         super().__init__()\n",
    "        \n",
    "#         layers = list()\n",
    "        \n",
    "#         for embed_dim in embed_dims:\n",
    "#             layers.append(torch.nn.Linear(input_dim, embed_dim))\n",
    "#             layers.append(torch.nn.BatchNorm1d(embed_dim))\n",
    "#             layers.append(torch.nn.ReLU())\n",
    "#             layers.append(torch.nn.Dropout(p=dropout))\n",
    "#             input_dim = embed_dim\n",
    "            \n",
    "#         if output_layer:\n",
    "#             layers.append(torch.nn.Linear(input_dim, 1))\n",
    "            \n",
    "#         self.mlp = torch.nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c510c4-b7c8-4129-a840-6d4158aeda9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomLinearRegression(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self, input_dimensions, output_dimensions):\n",
    "        \n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.linear = torch.nn.Linear(input_dimensions, \n",
    "#                                       output_dimensions)\n",
    "\n",
    "#     def forward(self, x): \n",
    "        \n",
    "#         result = self.linear(x.float())\n",
    "        \n",
    "#         squeezed_result = result.squeeze(1)\n",
    "        \n",
    "#         return squeezed_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
